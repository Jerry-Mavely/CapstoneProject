{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e32f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import subprocess\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Install pgeocode for geographic distance calculation\n",
    "try:\n",
    "    import pgeocode\n",
    "except ImportError:\n",
    "    print(\"Installing pgeocode...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pgeocode\"], check=True)\n",
    "    import pgeocode\n",
    "\n",
    "try:\n",
    "    from ethnicolr import census_ln\n",
    "except ImportError:\n",
    "    print(\"Installing ethnicolr...\")\n",
    "    # Note: ethnicolr requires TensorFlow. This installation might take a moment.\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"ethnicolr\"], check=True)\n",
    "    from ethnicolr import census_ln\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c01adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes loaded successfully.\n",
      "Patient DF shape: (100000, 16)\n",
      "Encounter DF shape: (200000, 17)\n",
      "Provider DF shape: (5000, 19)\n",
      "Hospital DF shape: (200, 14)\n"
     ]
    }
   ],
   "source": [
    "#file locations\n",
    "parquet_file_paths={\n",
    "    \"patient\": r\"Client_Data_files\\Parquets\\synthetic_patients.parquet\",\n",
    "    \"encounter\": r\"Client_Data_files\\Parquets\\synthetic_encounters.parquet\",\n",
    "    \"hospitals\": r\"Client_Data_files\\Parquets\\synthetic_hospitals.parquet\",\n",
    "    \"provider\": r\"Client_Data_files\\Parquets\\synthetic_providers.parquet\",    \n",
    "}\n",
    "\n",
    "# Reading the parquet files\n",
    "patient_df = pd.read_parquet(parquet_file_paths['patient'])\n",
    "encounter_df = pd.read_parquet(parquet_file_paths['encounter'])\n",
    "hospital_df = pd.read_parquet(parquet_file_paths['hospitals'])\n",
    "provider_df = pd.read_parquet(parquet_file_paths['provider'])\n",
    "\n",
    "print(\"Dataframes loaded successfully.\")\n",
    "print(f\"Patient DF shape: {patient_df.shape}\")\n",
    "print(f\"Encounter DF shape: {encounter_df.shape}\")\n",
    "print(f\"Provider DF shape: {provider_df.shape}\")\n",
    "print(f\"Hospital DF shape: {hospital_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208db927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe: patient\n",
      "Shape: (100000, 16)\n",
      "Columns: ['patient_id', 'first_name', 'last_name', 'date_of_birth', 'gender', 'race', 'ethnicity', 'primary_language', 'zip_code', 'insurance_type', 'household_income', 'education_level', 'age', 'cultural_background', 'preferred_provider_language', 'cultural_preferences']\n",
      "\n",
      "Dataframe: encounter\n",
      "Shape: (200000, 17)\n",
      "Columns: ['encounter_id', 'patient_id', 'provider_id', 'encounter_date', 'encounter_type', 'primary_diagnosis', 'length_of_stay', 'total_cost', 'cultural_background', 'primary_language', 'languages_spoken', 'cultural_competency_rating', 'cultural_match_score', 'language_match', 'patient_satisfaction', 'treatment_adherence', 'return_visit_30_days']\n",
      "\n",
      "Dataframe: hospitals\n",
      "Shape: (200, 14)\n",
      "Columns: ['hospital_id', 'hospital_name', 'hospital_type', 'zip_code', 'bed_count', 'teaching_hospital', 'trauma_center', 'language_services_available', 'cultural_competency_program', 'interpreter_services_24_7', 'community_health_programs', 'overall_rating', 'patient_safety_rating', 'readmission_rate']\n",
      "Column 'community_health_programs' has 121 missing values.\n",
      "\n",
      "Dataframe: provider\n",
      "Shape: (5000, 19)\n",
      "Columns: ['provider_id', 'npi_number', 'first_name', 'last_name', 'specialty', 'practice_zip_code', 'years_experience', 'medical_school_country', 'board_certified', 'languages_spoken', 'interpreter_services', 'cultural_certifications', 'minority_health_experience', 'community_involvement', 'patient_satisfaction_score', 'communication_rating', 'cultural_competency_rating', 'hospital_affiliation', 'accepts_new_patients']\n",
      "Column 'cultural_certifications' has 4056 missing values.\n",
      "Column 'community_involvement' has 4270 missing values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a data map for easy access\n",
    "data_map={\n",
    "    \"patient\": patient_df,\n",
    "    \"encounter\": encounter_df,\n",
    "    \"hospitals\": hospital_df,\n",
    "    \"provider\": provider_df\n",
    "}\n",
    "\n",
    "for key, df in data_map.items():\n",
    "    print(f\"Dataframe: {key}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")    \n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            print(f\"Column '{col}' has {df[col].isna().sum()} missing values.\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81358c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test print...\n"
     ]
    }
   ],
   "source": [
    "print(\"test print...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b1dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fcfc610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 17:11:11,774 - INFO - Preserving 12965 duplicate rows based on column 'last_name'\n",
      "2025-09-28 17:11:11,774 - INFO - Data filtering summary: 12997 → 12997 rows (kept 100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 17:11:11,783 - INFO - Merging demographic data for 12997 records...\n",
      "2025-09-28 17:11:11,847 - INFO - Matched 12997 of 12997 rows (100.0%)\n",
      "2025-09-28 17:11:11,848 - INFO - Added columns: pct2prace, pctaian, pctapi, pctblack, pcthispanic, pctwhite\n"
     ]
    }
   ],
   "source": [
    "patient_df_temp=pd.DataFrame()\n",
    "patient_df_temp=patient_df[patient_df['race']=='Hispanic or Latino'][['patient_id','first_name','last_name','race','ethnicity']].copy()\n",
    "patient_race_pred=census_ln(patient_df_temp, 'last_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa5ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5dc3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_mapping={\n",
    "    'white': 'White',\n",
    "    'black': 'Black or African American',\n",
    "    'api': 'Asian',    \n",
    "    'aian': 'Native American',\n",
    "    '2prace': 'Other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55a95de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_cols=['pctwhite','pctblack','pctapi','pctaian','pct2prace']\n",
    "patient_race_pred['derived_race'] = patient_race_pred[race_cols].idxmax(axis=1).str.replace('pct', '').map(race_mapping)\n",
    "\n",
    "# Create a mapping from patient_id to derived_race\n",
    "id_to_derived_race = dict(zip(patient_race_pred['patient_id'], patient_race_pred['derived_race']))\n",
    "\n",
    "# Update the race column only for Hispanic or Latino patients\n",
    "patient_df.loc[patient_df['race'] == 'Hispanic or Latino', 'race'] = \\\n",
    "    patient_df.loc[patient_df['race'] == 'Hispanic or Latino', 'patient_id'].map(id_to_derived_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1d3dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 17:11:56,058 - INFO - Preserving 4968 duplicate rows based on column 'last_name'\n",
      "2025-09-28 17:11:56,059 - INFO - Data filtering summary: 5000 → 5000 rows (kept 100.0%)\n",
      "2025-09-28 17:11:56,061 - INFO - Merging demographic data for 5000 records...\n",
      "2025-09-28 17:11:56,132 - INFO - Matched 5000 of 5000 rows (100.0%)\n",
      "2025-09-28 17:11:56,132 - INFO - Added columns: pct2prace, pctaian, pctapi, pctblack, pcthispanic, pctwhite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deriving race for providers from last names...\n",
      "Provider race derivation complete.\n",
      "Deriving ethnicity for providers from race predictions...\n",
      "Provider ethnicity derivation complete.\n"
     ]
    }
   ],
   "source": [
    "provider_race_predictions = census_ln(provider_df, 'last_name')\n",
    "\n",
    "# Derive race for the provider_df as it is missing from the source data\n",
    "print(\"Deriving race for providers from last names...\")\n",
    "race_cols = ['pctwhite','pctblack','pctapi','pctaian','pct2prace']\n",
    "provider_df['provider_race'] = provider_race_predictions[race_cols].idxmax(axis=1).str.replace('pct', '').map(race_mapping)\n",
    "print(\"Provider race derivation complete.\")\n",
    "\n",
    "# Deriving provider ethnicity from the race_predictions\n",
    "print(\"Deriving ethnicity for providers from race predictions...\")\n",
    "provider_df['provider_ethnicity'] = provider_race_predictions['pcthispanic'].apply(lambda x: 'Hispanic or Latino' if float(x) >= 50 else 'Not Hispanic or Latino')\n",
    "print(\"Provider ethnicity derivation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcf27b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe: patient\n",
      "Shape: (100000, 16)\n",
      "Columns: ['patient_id', 'first_name', 'last_name', 'date_of_birth', 'gender', 'race', 'ethnicity', 'primary_language', 'zip_code', 'insurance_type', 'household_income', 'education_level', 'age', 'cultural_background', 'preferred_provider_language', 'cultural_preferences']\n",
      "\n",
      "Dataframe: encounter\n",
      "Shape: (200000, 17)\n",
      "Columns: ['encounter_id', 'patient_id', 'provider_id', 'encounter_date', 'encounter_type', 'primary_diagnosis', 'length_of_stay', 'total_cost', 'cultural_background', 'primary_language', 'languages_spoken', 'cultural_competency_rating', 'cultural_match_score', 'language_match', 'patient_satisfaction', 'treatment_adherence', 'return_visit_30_days']\n",
      "\n",
      "Dataframe: hospitals\n",
      "Shape: (200, 14)\n",
      "Columns: ['hospital_id', 'hospital_name', 'hospital_type', 'zip_code', 'bed_count', 'teaching_hospital', 'trauma_center', 'language_services_available', 'cultural_competency_program', 'interpreter_services_24_7', 'community_health_programs', 'overall_rating', 'patient_safety_rating', 'readmission_rate']\n",
      "Column 'community_health_programs' has 121 missing values.\n",
      "\n",
      "Dataframe: provider\n",
      "Shape: (5000, 21)\n",
      "Columns: ['provider_id', 'npi_number', 'first_name', 'last_name', 'specialty', 'practice_zip_code', 'years_experience', 'medical_school_country', 'board_certified', 'languages_spoken', 'interpreter_services', 'cultural_certifications', 'minority_health_experience', 'community_involvement', 'patient_satisfaction_score', 'communication_rating', 'cultural_competency_rating', 'hospital_affiliation', 'accepts_new_patients', 'provider_race', 'provider_ethnicity']\n",
      "Column 'cultural_certifications' has 4056 missing values.\n",
      "Column 'community_involvement' has 4270 missing values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, df in data_map.items():\n",
    "    print(f\"Dataframe: {key}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")    \n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            print(f\"Column '{col}' has {df[col].isna().sum()} missing values.\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381511fe",
   "metadata": {},
   "source": [
    "### Step 2: Feature Engineering\n",
    "\n",
    "Here, we merge the datasets and create the features our model will learn from. This includes cultural matches, language matches, and geographic distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "564990c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master DataFrame created with shape: (200000, 66)\n"
     ]
    }
   ],
   "source": [
    "# Merge all data into a single master DataFrame for training\n",
    "master_df = pd.merge(encounter_df, patient_df, on='patient_id',suffixes=('', '_pat'))\n",
    "master_df = pd.merge(master_df, provider_df, on='provider_id',suffixes=('', '_prov'))\n",
    "master_df = pd.merge(master_df, hospital_df, left_on='hospital_affiliation', right_on='hospital_id',suffixes=('', '_hosp'))\n",
    "\n",
    "print(\"Master DataFrame created with shape:\", master_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8e7604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['encounter_id', 'patient_id', 'provider_id', 'encounter_date',\n",
       "       'encounter_type', 'primary_diagnosis', 'length_of_stay', 'total_cost',\n",
       "       'cultural_background', 'primary_language', 'languages_spoken',\n",
       "       'cultural_competency_rating', 'cultural_match_score', 'language_match',\n",
       "       'patient_satisfaction', 'treatment_adherence', 'return_visit_30_days',\n",
       "       'first_name', 'last_name', 'date_of_birth', 'gender', 'race',\n",
       "       'ethnicity', 'primary_language_pat', 'zip_code', 'insurance_type',\n",
       "       'household_income', 'education_level', 'age', 'cultural_background_pat',\n",
       "       'preferred_provider_language', 'cultural_preferences', 'npi_number',\n",
       "       'first_name_prov', 'last_name_prov', 'specialty', 'practice_zip_code',\n",
       "       'years_experience', 'medical_school_country', 'board_certified',\n",
       "       'languages_spoken_prov', 'interpreter_services',\n",
       "       'cultural_certifications', 'minority_health_experience',\n",
       "       'community_involvement', 'patient_satisfaction_score',\n",
       "       'communication_rating', 'cultural_competency_rating_prov',\n",
       "       'hospital_affiliation', 'accepts_new_patients', 'provider_race',\n",
       "       'provider_ethnicity', 'hospital_id', 'hospital_name', 'hospital_type',\n",
       "       'zip_code_hosp', 'bed_count', 'teaching_hospital', 'trauma_center',\n",
       "       'language_services_available', 'cultural_competency_program',\n",
       "       'interpreter_services_24_7', 'community_health_programs',\n",
       "       'overall_rating', 'patient_safety_rating', 'readmission_rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a664f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          True\n",
       "1          True\n",
       "2          True\n",
       "3          True\n",
       "4          True\n",
       "          ...  \n",
       "199995     True\n",
       "199996     True\n",
       "199997    False\n",
       "199998     True\n",
       "199999     True\n",
       "Name: language_match, Length: 200000, dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "master_df['language_match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd9d9485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jerry\\AppData\\Local\\Temp\\ipykernel_30708\\364688356.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  master_df['distance_km'].fillna(mean_dist_by_specialty, inplace=True)\n",
      "C:\\Users\\jerry\\AppData\\Local\\Temp\\ipykernel_30708\\364688356.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  master_df['distance_km'].fillna(master_df['distance_km'].mean(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_match</th>\n",
       "      <th>ethnicity_match</th>\n",
       "      <th>language_match</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>proximity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>653.678558</td>\n",
       "      <td>0.959939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1869.879967</td>\n",
       "      <td>0.885402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1792.544564</td>\n",
       "      <td>0.890142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>983.320368</td>\n",
       "      <td>0.939736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>444.378187</td>\n",
       "      <td>0.972766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   race_match  ethnicity_match  language_match  distance_km  proximity_score\n",
       "0           1                1               1   653.678558         0.959939\n",
       "1           0                0               1  1869.879967         0.885402\n",
       "2           0                1               1  1792.544564         0.890142\n",
       "3           1                1               1   983.320368         0.939736\n",
       "4           1                1               1   444.378187         0.972766"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Engineer the Match Features ---\n",
    "\n",
    "# Cultural Features\n",
    "master_df['race_match'] = (master_df['race'] == master_df['provider_race']).astype(int)\n",
    "master_df['ethnicity_match'] = (master_df['ethnicity'] == master_df['provider_ethnicity']).astype(int)\n",
    "\n",
    "# Language Feature\n",
    "# # The 'languages_spoken' column is a string like '[\"English\", \"Spanish\"]'. We need to parse it.\n",
    "# import ast\n",
    "# master_df['languages_spoken_list'] = master_df['languages_spoken'].apply(ast.literal_eval)\n",
    "master_df['language_match'] = (master_df['language_match'] == True).astype(int)\n",
    "\n",
    "# Geographic Feature\n",
    "dist = pgeocode.GeoDistance('US') # Assuming US zip codes\n",
    "# Calculate distance between patient and provider zip codes\n",
    "master_df['distance_km'] = dist.query_postal_code(\n",
    "    master_df['zip_code'].astype(str).tolist(), \n",
    "    master_df['zip_code_hosp'].astype(str).tolist()\n",
    ")\n",
    "# Calculate the mean distance for each provider specialty\n",
    "# The .transform('mean') creates a Series with the same index as master_df,\n",
    "mean_dist_by_specialty = master_df.groupby('specialty')['distance_km'].transform('mean')\n",
    "\n",
    "# Now, fill the missing distances using these specialty-specific averages\n",
    "master_df['distance_km'].fillna(mean_dist_by_specialty, inplace=True)\n",
    "\n",
    "# If any specialties had NO valid distances, there might still be NaNs.\n",
    "# Fill any remaining with the overall mean as a final fallback.\n",
    "master_df['distance_km'].fillna(master_df['distance_km'].mean(), inplace=True)\n",
    "\n",
    "# 1. First, calculate the min and max distances from data\n",
    "min_distance = master_df['distance_km'].min()\n",
    "max_distance = master_df['distance_km'].max()\n",
    "\n",
    "# 2. Apply the scaling formula to create a score from 0 to 1\n",
    "# The formula is: 1 - ( (x - min) / (max - min) )\n",
    "\n",
    "master_df['proximity_score'] = 1 - (\n",
    "    (master_df['distance_km'] - min_distance) / (max_distance - min_distance)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Feature engineering complete.\")\n",
    "master_df[['race_match', 'ethnicity_match', 'language_match', 'distance_km','proximity_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b75b5356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    200000.000000\n",
       "mean          0.892787\n",
       "std           0.034057\n",
       "min           0.000000\n",
       "25%           0.890142\n",
       "50%           0.893621\n",
       "75%           0.894823\n",
       "max           1.000000\n",
       "Name: proximity_score, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(master_df['proximity_score'].isna().sum())\n",
    "master_df['proximity_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad339703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_df.to_csv('master_df_v1_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c06a3",
   "metadata": {},
   "source": [
    "### Step 3: Training and Testing the Model\n",
    "\n",
    "This is the core machine learning section. We split our data, train the model, and then test it on unseen data to validate its performance. The feature importances are the **learned weights**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29dec8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique cultural preferences found:  ['No Specific Preference' 'Culturally Similar Provider'\n",
      " 'Culturally Similar Provider; Same Language Provider'\n",
      " 'Same Language Provider']\n"
     ]
    }
   ],
   "source": [
    "# --- Training Section ---\n",
    "\n",
    "# 1. Normalize the columns to a 0-1 scale\n",
    "scaler = MinMaxScaler()\n",
    "master_df[['satisfaction_norm', 'adherence_norm']] = scaler.fit_transform(\n",
    "    master_df[['patient_satisfaction', 'treatment_adherence']]\n",
    ")\n",
    "\n",
    "# 2. Define weights and create the composite score\n",
    "adherence_weight = 0.5\n",
    "satisfaction_weight = 0.5\n",
    "master_df['success_score'] = (\n",
    "    master_df['adherence_norm'] * adherence_weight +\n",
    "    master_df['satisfaction_norm'] * satisfaction_weight\n",
    ")\n",
    "\n",
    "# Rename provider competency column to avoid conflict\n",
    "master_df.rename(columns={'cultural_competency_rating_y': 'cultural_competency_rating_prov'}, inplace=True)\n",
    "\n",
    "# Define the features to be used by the models\n",
    "features = [\n",
    "    'years_experience',\n",
    "    'cultural_competency_rating_prov',\n",
    "    'communication_rating',\n",
    "    'race_match',\n",
    "    'ethnicity_match',\n",
    "    'language_match',\n",
    "    'proximity_score',\n",
    "    'interpreter_services_24_7'\n",
    "]\n",
    "target = 'success_score'\n",
    "\n",
    "# Get the unique preference categories to loop through\n",
    "unique_preferences = master_df['cultural_preferences'].unique()\n",
    "print(\"Unique cultural preferences found: \", unique_preferences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7e7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for cultural preference: No Specific Preference (1/4)--------------------------------------\n",
      "Starting hyperparameter tuning on 69151 samples...\n",
      "Tuning complete. Best parameters found:\n",
      "{'n_estimators': 200, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n",
      "\n",
      "--- Final Evaluation on the Held-Out Test Set ---\n",
      "Test set size: 17288 encounters\n",
      "\n",
      "--- Final Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1095\n",
      "Mean Absolute Error (MAE):     0.0906\n",
      "R-squared (R²):                -0.0002\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "cultural_competency_rating_prov    0.302281\n",
      "proximity_score                    0.258730\n",
      "communication_rating               0.227062\n",
      "years_experience                   0.150782\n",
      "interpreter_services_24_7          0.023370\n",
      "race_match                         0.019298\n",
      "ethnicity_match                    0.018477\n",
      "language_match                     0.000000\n",
      "dtype: float64\n",
      "\n",
      "--- Best model for 'No Specific Preference' trained and stored ----------------------------------------------------------------\n",
      "Training model for cultural preference: Culturally Similar Provider (2/4)--------------------------------------\n",
      "Starting hyperparameter tuning on 55405 samples...\n",
      "Tuning complete. Best parameters found:\n",
      "{'n_estimators': 150, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10}\n",
      "\n",
      "--- Final Evaluation on the Held-Out Test Set ---\n",
      "Test set size: 13852 encounters\n",
      "\n",
      "--- Final Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1092\n",
      "Mean Absolute Error (MAE):     0.0902\n",
      "R-squared (R²):                -0.0024\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "cultural_competency_rating_prov    0.279446\n",
      "communication_rating               0.249643\n",
      "proximity_score                    0.236338\n",
      "years_experience                   0.164338\n",
      "race_match                         0.024942\n",
      "ethnicity_match                    0.023974\n",
      "interpreter_services_24_7          0.021318\n",
      "language_match                     0.000000\n",
      "dtype: float64\n",
      "\n",
      "--- Best model for 'Culturally Similar Provider' trained and stored ----------------------------------------------------------------\n",
      "Training model for cultural preference: Culturally Similar Provider; Same Language Provider (3/4)--------------------------------------\n",
      "Starting hyperparameter tuning on 15664 samples...\n",
      "Tuning complete. Best parameters found:\n",
      "{'n_estimators': 200, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n",
      "\n",
      "--- Final Evaluation on the Held-Out Test Set ---\n",
      "Test set size: 3916 encounters\n",
      "\n",
      "--- Final Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1402\n",
      "Mean Absolute Error (MAE):     0.1151\n",
      "R-squared (R²):                0.3684\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "language_match                     0.829199\n",
      "cultural_competency_rating_prov    0.061655\n",
      "communication_rating               0.039800\n",
      "proximity_score                    0.030068\n",
      "years_experience                   0.025685\n",
      "ethnicity_match                    0.004904\n",
      "race_match                         0.004583\n",
      "interpreter_services_24_7          0.004105\n",
      "dtype: float64\n",
      "\n",
      "--- Best model for 'Culturally Similar Provider; Same Language Provider' trained and stored ----------------------------------------------------------------\n",
      "Training model for cultural preference: Same Language Provider (4/4)--------------------------------------\n",
      "Starting hyperparameter tuning on 19779 samples...\n",
      "Tuning complete. Best parameters found:\n",
      "{'n_estimators': 200, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 10}\n",
      "\n",
      "--- Final Evaluation on the Held-Out Test Set ---\n",
      "Test set size: 4945 encounters\n",
      "\n",
      "--- Final Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1397\n",
      "Mean Absolute Error (MAE):     0.1139\n",
      "R-squared (R²):                0.3727\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "language_match                     0.879990\n",
      "cultural_competency_rating_prov    0.043315\n",
      "communication_rating               0.027713\n",
      "proximity_score                    0.024068\n",
      "years_experience                   0.017200\n",
      "interpreter_services_24_7          0.002809\n",
      "race_match                         0.002541\n",
      "ethnicity_match                    0.002364\n",
      "dtype: float64\n",
      "\n",
      "--- Best model for 'Same Language Provider' trained and stored ----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# A dictionary to store a trained model for each preference type\n",
    "trained_models = {}\n",
    "learned_weights_dict = {}\n",
    "best_hyperparameters = {} \n",
    "test_metrics_dict = {}\n",
    "\n",
    "for i, preference in enumerate(unique_preferences):\n",
    "    print(f\"Training model for cultural preference: {preference} ({i+1}/{len(unique_preferences)})--------------------------------------\")\n",
    "    \n",
    "    # Filter the DataFrame for the current preference\n",
    "    segment_df= master_df[master_df['cultural_preferences'] == preference].copy()\n",
    "\n",
    "    # Check if the segment is large enough to train a model\n",
    "    if len(segment_df) < 100: # You can adjust this threshold\n",
    "        print(f\"Segment is too small to train a reliable model. Skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_segment = segment_df[features]\n",
    "    y_segment = segment_df[target]\n",
    "\n",
    "    # 1. Split data into a training+validation set (80%) and a final test set (20%)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X_segment, y_segment, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # --- Hyperparameter Tuning Section (using the validation data implicitly via CV) ---\n",
    "\n",
    "    # 2. Define the grid of hyperparameters to search\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', 1.0]\n",
    "    }\n",
    "\n",
    "    # 3. Set up Randomized Search with 5-fold cross-validation\n",
    "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=rf, \n",
    "        param_distributions=param_grid, \n",
    "        n_iter=50,  # Number of parameter settings that are sampled\n",
    "        cv=5,       # 5-fold cross-validation\n",
    "        verbose=0,  # Set to 1 or 2 for more detailed output\n",
    "        random_state=42, \n",
    "        scoring='neg_mean_squared_error' # We use negative MSE for optimization\n",
    "    )\n",
    "\n",
    "    print(f'Starting hyperparameter tuning on {len(X_train_val)} samples...')\n",
    "    # 4. Fit the random search to the training+validation data\n",
    "    random_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "    print(f'Tuning complete. Best parameters found:')\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "    # 5. Get the best model found during the search\n",
    "    best_hyperparameters[preference] = random_search.best_params_ # <-- NEW LINE: Store the best params\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # --- Final Testing Section ---\n",
    "\n",
    "    print(\"\\n--- Final Evaluation on the Held-Out Test Set ---\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} encounters\")\n",
    "    \n",
    "    # 6. Make predictions on the unseen test data\n",
    "    final_predictions = best_model.predict(X_test)\n",
    "\n",
    "    # 7. Evaluate the final model's performance\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
    "    mae = mean_absolute_error(y_test, final_predictions)\n",
    "    r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "    print(\"\\n--- Final Model Validation Metrics ---\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE):     {mae:.4f}\")\n",
    "    print(f\"R-squared (R²):                {r2:.4f}\")\n",
    "\n",
    "    # store the test run results \n",
    "    # You can store these metrics in a dictionary or DataFrame if needed\n",
    "    test_metrics = {\n",
    "        'preference': preference,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'test_set_size': X_test.shape[0]\n",
    "    }\n",
    "\n",
    "    # 8. Inspect and store the results from the best model\n",
    "    learned_weights = pd.Series(best_model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "    print(\"\\n--- Learned Feature Weights for this Segment ---\")\n",
    "    print(learned_weights)\n",
    "\n",
    "    test_metrics_dict[preference] = test_metrics\n",
    "    trained_models[preference] = best_model\n",
    "    learned_weights_dict[preference] = learned_weights\n",
    "    print(f\"\\n--- Best model for '{preference}' trained and stored ----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0908fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Specific Preference</th>\n",
       "      <th>Culturally Similar Provider</th>\n",
       "      <th>Culturally Similar Provider; Same Language Provider</th>\n",
       "      <th>Same Language Provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_features</th>\n",
       "      <td>log2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>log2</td>\n",
       "      <td>log2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 No Specific Preference Culturally Similar Provider  \\\n",
       "n_estimators                        200                         150   \n",
       "min_samples_leaf                      4                           4   \n",
       "max_features                       log2                        sqrt   \n",
       "max_depth                            10                          10   \n",
       "\n",
       "                 Culturally Similar Provider; Same Language Provider  \\\n",
       "n_estimators                                                    200    \n",
       "min_samples_leaf                                                  4    \n",
       "max_features                                                   log2    \n",
       "max_depth                                                        10    \n",
       "\n",
       "                 Same Language Provider  \n",
       "n_estimators                        200  \n",
       "min_samples_leaf                      4  \n",
       "max_features                       log2  \n",
       "max_depth                            10  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_df = pd.DataFrame(learned_weights_dict).reset_index(drop=False).rename(columns={'index': 'feature'})\n",
    "hyperparams_df = pd.DataFrame(best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models trained and stored: 4\n",
      "learned_weights_dict keys: dict_keys(['No Specific Preference', 'Culturally Similar Provider', 'Culturally Similar Provider; Same Language Provider', 'Same Language Provider'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total models trained and stored: {len(trained_models)}\")\n",
    "print(f'learned_weights_dict keys: {learned_weights_dict.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca1627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique cultural preferences found:  ['No Specific Preference' 'Culturally Similar Provider'\n",
      " 'Culturally Similar Provider; Same Language Provider'\n",
      " 'Same Language Provider']\n",
      "preference segment 1 of 4\n",
      "Processing Segment: 'No Specific Preference' ---\n",
      "Training set size: 69151 encounters\n",
      "Testing set size: 17288 encounters\n",
      "Training set size: 69151 encounters\n",
      "Testing set size: 17288 encounters\n",
      "\n",
      "--- Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1151\n",
      "Mean Absolute Error (MAE):     0.0946\n",
      "R-squared (R²):                -0.1067\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "proximity_score                    0.386974\n",
      "communication_rating               0.230190\n",
      "cultural_competency_rating_prov    0.164537\n",
      "years_experience                   0.143111\n",
      "interpreter_services_24_7          0.027562\n",
      "race_match                         0.025218\n",
      "ethnicity_match                    0.022407\n",
      "language_match                     0.000000\n",
      "dtype: float64\n",
      "--- Model for 'No Specific Preference' trained and stored ---\n",
      "\n",
      "preference segment 2 of 4\n",
      "Processing Segment: 'Culturally Similar Provider' ---\n",
      "\n",
      "--- Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1151\n",
      "Mean Absolute Error (MAE):     0.0946\n",
      "R-squared (R²):                -0.1067\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "proximity_score                    0.386974\n",
      "communication_rating               0.230190\n",
      "cultural_competency_rating_prov    0.164537\n",
      "years_experience                   0.143111\n",
      "interpreter_services_24_7          0.027562\n",
      "race_match                         0.025218\n",
      "ethnicity_match                    0.022407\n",
      "language_match                     0.000000\n",
      "dtype: float64\n",
      "--- Model for 'No Specific Preference' trained and stored ---\n",
      "\n",
      "preference segment 2 of 4\n",
      "Processing Segment: 'Culturally Similar Provider' ---\n",
      "Training set size: 55405 encounters\n",
      "Testing set size: 13852 encounters\n",
      "Training set size: 55405 encounters\n",
      "Testing set size: 13852 encounters\n",
      "\n",
      "--- Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1221\n",
      "Mean Absolute Error (MAE):     0.0995\n",
      "R-squared (R²):                -0.2528\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "proximity_score                    0.236383\n",
      "communication_rating               0.229897\n",
      "cultural_competency_rating_prov    0.165198\n",
      "years_experience                   0.151827\n",
      "ethnicity_match                    0.094770\n",
      "race_match                         0.093170\n",
      "interpreter_services_24_7          0.028755\n",
      "language_match                     0.000000\n",
      "dtype: float64\n",
      "--- Model for 'Culturally Similar Provider' trained and stored ---\n",
      "\n",
      "preference segment 3 of 4\n",
      "Processing Segment: 'Culturally Similar Provider; Same Language Provider' ---\n",
      "Training set size: 15664 encounters\n",
      "Testing set size: 3916 encounters\n",
      "\n",
      "--- Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1221\n",
      "Mean Absolute Error (MAE):     0.0995\n",
      "R-squared (R²):                -0.2528\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "proximity_score                    0.236383\n",
      "communication_rating               0.229897\n",
      "cultural_competency_rating_prov    0.165198\n",
      "years_experience                   0.151827\n",
      "ethnicity_match                    0.094770\n",
      "race_match                         0.093170\n",
      "interpreter_services_24_7          0.028755\n",
      "language_match                     0.000000\n",
      "dtype: float64\n",
      "--- Model for 'Culturally Similar Provider' trained and stored ---\n",
      "\n",
      "preference segment 3 of 4\n",
      "Processing Segment: 'Culturally Similar Provider; Same Language Provider' ---\n",
      "Training set size: 15664 encounters\n",
      "Testing set size: 3916 encounters\n",
      "\n",
      "--- Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1561\n",
      "Mean Absolute Error (MAE):     0.1266\n",
      "R-squared (R²):                0.2166\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "language_match                     0.405166\n",
      "communication_rating               0.156204\n",
      "proximity_score                    0.118309\n",
      "cultural_competency_rating_prov    0.114936\n",
      "years_experience                   0.097951\n",
      "ethnicity_match                    0.045206\n",
      "race_match                         0.043517\n",
      "interpreter_services_24_7          0.018711\n",
      "dtype: float64\n",
      "--- Model for 'Culturally Similar Provider; Same Language Provider' trained and stored ---\n",
      "\n",
      "preference segment 4 of 4\n",
      "Processing Segment: 'Same Language Provider' ---\n",
      "Training set size: 19779 encounters\n",
      "Testing set size: 4945 encounters\n",
      "\n",
      "--- Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1561\n",
      "Mean Absolute Error (MAE):     0.1266\n",
      "R-squared (R²):                0.2166\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "language_match                     0.405166\n",
      "communication_rating               0.156204\n",
      "proximity_score                    0.118309\n",
      "cultural_competency_rating_prov    0.114936\n",
      "years_experience                   0.097951\n",
      "ethnicity_match                    0.045206\n",
      "race_match                         0.043517\n",
      "interpreter_services_24_7          0.018711\n",
      "dtype: float64\n",
      "--- Model for 'Culturally Similar Provider; Same Language Provider' trained and stored ---\n",
      "\n",
      "preference segment 4 of 4\n",
      "Processing Segment: 'Same Language Provider' ---\n",
      "Training set size: 19779 encounters\n",
      "Testing set size: 4945 encounters\n",
      "\n",
      "--- Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1547\n",
      "Mean Absolute Error (MAE):     0.1256\n",
      "R-squared (R²):                0.2301\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "language_match                     0.520861\n",
      "communication_rating               0.132122\n",
      "proximity_score                    0.128981\n",
      "cultural_competency_rating_prov    0.097229\n",
      "years_experience                   0.082057\n",
      "interpreter_services_24_7          0.014511\n",
      "race_match                         0.012325\n",
      "ethnicity_match                    0.011913\n",
      "dtype: float64\n",
      "--- Model for 'Same Language Provider' trained and stored ---\n",
      "\n",
      "\n",
      "--- Model Validation Metrics ---\n",
      "Root Mean Squared Error (RMSE): 0.1547\n",
      "Mean Absolute Error (MAE):     0.1256\n",
      "R-squared (R²):                0.2301\n",
      "\n",
      "--- Learned Feature Weights for this Segment ---\n",
      "language_match                     0.520861\n",
      "communication_rating               0.132122\n",
      "proximity_score                    0.128981\n",
      "cultural_competency_rating_prov    0.097229\n",
      "years_experience                   0.082057\n",
      "interpreter_services_24_7          0.014511\n",
      "race_match                         0.012325\n",
      "ethnicity_match                    0.011913\n",
      "dtype: float64\n",
      "--- Model for 'Same Language Provider' trained and stored ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # --- Training Section ---\n",
    "\n",
    "# # 1. Normalize the columns to a 0-1 scale\n",
    "# scaler = MinMaxScaler()\n",
    "# master_df[['satisfaction_norm', 'adherence_norm']] = scaler.fit_transform(\n",
    "#     master_df[['patient_satisfaction', 'treatment_adherence']]\n",
    "# )\n",
    "\n",
    "# # 2. Define weights and create the composite score\n",
    "# adherence_weight = 0.5\n",
    "# satisfaction_weight = 0.5\n",
    "# master_df['success_score'] = (\n",
    "#     master_df['adherence_norm'] * adherence_weight +\n",
    "#     master_df['satisfaction_norm'] * satisfaction_weight\n",
    "# )\n",
    "\n",
    "# # Rename provider competency column to avoid conflict\n",
    "# master_df.rename(columns={'cultural_competency_rating_y': 'cultural_competency_rating_prov'}, inplace=True)\n",
    "\n",
    "# # Define the features to be used by the models\n",
    "# features = [\n",
    "#     'years_experience',\n",
    "#     'cultural_competency_rating_prov',\n",
    "#     'communication_rating',\n",
    "#     'race_match',\n",
    "#     'ethnicity_match',\n",
    "#     'language_match',\n",
    "#     'proximity_score',\n",
    "#     'interpreter_services_24_7'\n",
    "# ]\n",
    "# target = 'success_score'\n",
    "\n",
    "# # --- Segmented Training and Testing ---\n",
    "\n",
    "# # Get the unique preference categories to loop through\n",
    "# unique_preferences = master_df['cultural_preferences'].unique()\n",
    "# print(\"Unique cultural preferences found: \", unique_preferences)\n",
    "\n",
    "# # A dictionary to store a trained model for each preference type\n",
    "# trained_models = {}\n",
    "# learned_weights_dict = {}\n",
    "\n",
    "# i=1\n",
    " \n",
    "# for preference in unique_preferences:\n",
    "#     print(f'preference segment {i} of {len(unique_preferences)}')\n",
    "#     print(f\"Processing Segment: '{preference}' ---\")\n",
    "    \n",
    "#     # Create a subset of the data for the current preference type\n",
    "#     segment_df = master_df[master_df['cultural_preferences'] == preference].copy()\n",
    "    \n",
    "#     # Check if the segment is large enough to train a model\n",
    "#     if len(segment_df) < 50: # You can adjust this threshold\n",
    "#         print(f\"Segment is too small to train a reliable model. Skipping.\\n\")\n",
    "#         continue\n",
    "\n",
    "#     # --- Training Section for the Segment ---\n",
    "    \n",
    "#     X_segment = segment_df[features]\n",
    "#     y_segment = segment_df[target]\n",
    "\n",
    "#     # 1. Split the segment's data into a training set (80%) and a testing set (20%)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_segment, y_segment, test_size=0.2, random_state=42)\n",
    "\n",
    "#     print(f\"Training set size: {X_train.shape[0]} encounters\")\n",
    "#     print(f\"Testing set size: {X_test.shape[0]} encounters\")\n",
    "\n",
    "#     # 2. Initialize and train a new Random Forest model for this segment\n",
    "#     model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     # --- Testing Section for the Segment ---\n",
    "\n",
    "#     # 3. Make predictions on the unseen test data for this segment\n",
    "#     predictions = model.predict(X_test)\n",
    "\n",
    "#     # 4. Evaluate the model's performance\n",
    "#     rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "#     mae = mean_absolute_error(y_test, predictions)\n",
    "#     r2 = r2_score(y_test, predictions)\n",
    "\n",
    "#     print(\"\\n--- Model Validation Metrics ---\")\n",
    "#     print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "#     print(f\"Mean Absolute Error (MAE):     {mae:.4f}\")\n",
    "#     print(f\"R-squared (R²):                {r2:.4f}\")\n",
    "\n",
    "#     # 5. Inspect the 'Learned Weights' (Feature Importances) for this specific model\n",
    "#     learned_weights = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "#     print(\"\\n--- Learned Feature Weights for this Segment ---\")\n",
    "#     print(learned_weights)\n",
    "    \n",
    "#     # 6. Store the trained model\n",
    "#     trained_models[preference] = model\n",
    "#     learned_weights_dict[preference] = learned_weights\n",
    "#     print(f\"--- Model for '{preference}' trained and stored ---\\n\")\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb16a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to dataframe\n",
    "learned_weights_df = pd.DataFrame(learned_weights)\n",
    "learned_weights_df.index.name = 'cultural_preferences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e712f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference: No Specific Preference\n",
      "[0.386974489821484, 0.23019002327162547, 0.16453684159692655, 0.14311135388525828, 0.02756165400768089, 0.025218383833930574, 0.022407253583094255, 0.0]\n",
      "Preference: Culturally Similar Provider\n",
      "[0.23638293508171723, 0.22989727272492963, 0.16519773576037672, 0.15182700676172423, 0.09477007405746572, 0.09317003986773503, 0.028754935746051466, 0.0]\n",
      "Preference: Culturally Similar Provider; Same Language Provider\n",
      "[0.4051664254176326, 0.15620424810482433, 0.11830900818510516, 0.11493618900534715, 0.09795053882928924, 0.04520594833335658, 0.043516902229710705, 0.01871073989473434]\n",
      "Preference: Same Language Provider\n",
      "[0.5208611621416773, 0.13212231087597104, 0.1289808596215443, 0.09722874995700181, 0.0820571080118914, 0.014511249376409916, 0.012325471480068885, 0.011913088535435318]\n"
     ]
    }
   ],
   "source": [
    "for key in learned_weights_dict.keys():\n",
    "    print(f\"Preference: {key}\")\n",
    "    print(learned_weights_dict[key].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a75a37",
   "metadata": {},
   "source": [
    "# Convert learned_weights dictionary to a DataFrame\n",
    "learned_weights_df = pd.DataFrame(learned_weights)\n",
    "learned_weights_df.index.name = 'feature'\n",
    "display(learned_weights_df)mendations for a new patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(patient_id, required_specialty, \n",
    "                        # DataFrames\n",
    "                        all_providers_df, \n",
    "                        all_patients_df, \n",
    "                        all_hospitals_df,\n",
    "                        # Trained Models & Scaling Data\n",
    "                        models_dict,\n",
    "                        training_min_dist, \n",
    "                        training_max_dist):\n",
    "    \"\"\"\n",
    "    Generates ranked doctor recommendations using the segmented model strategy.\n",
    "    \n",
    "    Args:\n",
    "        patient_id (int): The ID of the patient seeking a recommendation.\n",
    "        required_specialty (str): The medical specialty required.\n",
    "        all_providers_df (pd.DataFrame): The full provider dataframe.\n",
    "        all_patients_df (pd.DataFrame): The full patient dataframe.\n",
    "        all_hospitals_df (pd.DataFrame): The full hospital dataframe.\n",
    "        models_dict (dict): The dictionary of trained models for each preference segment.\n",
    "        training_min_dist (float): The minimum distance calculated from the full training set.\n",
    "        training_max_dist (float): The maximum distance calculated from the full training set.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A ranked dataframe of recommended providers.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Recommendation Phase for Patient ID: {patient_id} ---\")\n",
    "    \n",
    "    # --- Step 1: Patient Lookup & Model Routing ---\n",
    "    patient_info = all_patients_df[all_patients_df['patient_id'] == patient_id]\n",
    "    if patient_info.empty:\n",
    "        return \"Error: Patient ID not found.\"\n",
    "        \n",
    "    preference = patient_info['cultural_preferences'].iloc[0]\n",
    "    model_to_use = None\n",
    "    model_name = \"\"\n",
    "\n",
    "    # Determine which trained model to use based on the patient's preference\n",
    "    if preference == 'Culturally Similar Provider; Same Language Provider':\n",
    "        model_name = 'Culturally Similar Provider; Same Language Provider' # Match the exact key from training\n",
    "    elif preference == 'Culturally Similar Provider':\n",
    "        model_name = 'Culturally Similar Provider'\n",
    "    elif preference == 'Same Language Provider':\n",
    "        model_name = 'Same Language Provider'\n",
    "    else: # Fallback for any other preferences, like just \"Same Language Provider\"\n",
    "        model_name = 'No Specific Preference'\n",
    "\n",
    "    model_to_use = models_dict.get(model_name)\n",
    "    print(f\"Patient preference: '{preference}'. Routing to '{model_name}' model.\")\n",
    "        \n",
    "    if not model_to_use:\n",
    "        return f\"Error: Model for preference group '{model_name}' was not trained (likely due to small size).\"\n",
    "\n",
    "\n",
    "    # --- Step 2: Candidate Generation (Hard Filters) ---\n",
    "    candidate_providers_accepting_new = all_providers_df[all_providers_df['accepts_new_patients'] == True]\n",
    "    previous_provider=encounter_df[encounter_df['patient_id']==patient_id]['provider_id'].unique()\n",
    "    candidate_providers_not_accepting_new = all_providers_df[\n",
    "        (all_providers_df['accepts_new_patients'] == False) &\n",
    "        (all_providers_df['provider_id'].isin(previous_provider))\n",
    "    ]\n",
    "    \n",
    "    candidate_providers_all=pd.concat([candidate_providers_accepting_new,candidate_providers_not_accepting_new])\n",
    "\n",
    "    candidate_providers = candidate_providers_all[\n",
    "        (candidate_providers_all['specialty'] == required_specialty)         \n",
    "    ].copy()\n",
    "    if candidate_providers.empty:\n",
    "        print(f\"No providers found for specialty '{required_specialty}' \")\n",
    "        candidate_providers = candidate_providers_all\n",
    "\n",
    "    # --- Step 3: Feature Engineering for Inference ---\n",
    "    # Merge all necessary info for the patient and candidate providers\n",
    "    inference_df = candidate_providers.assign(key=1).merge(patient_info.assign(key=1), on='key').drop('key', axis=1)\n",
    "    inference_df = pd.merge(inference_df, all_hospitals_df, left_on='hospital_affiliation', right_on='hospital_id', how='left')\n",
    "\n",
    "    print(f'inference_df columns: {inference_df.columns.tolist()}')\n",
    "\n",
    "    # Re-create the exact same features used in training\n",
    "    inference_df.rename(columns={'cultural_competency_rating_y': 'cultural_competency_rating_prov'}, inplace=True)\n",
    "    inference_df['race_match'] = (inference_df['race'] == inference_df['provider_race']).astype(int)\n",
    "    inference_df['ethnicity_match'] = (inference_df['ethnicity'] == inference_df['provider_ethnicity']).astype(int)\n",
    "\n",
    "    # create language match if preferred_provider_language in languages_spoken (semicolon-separated string)\n",
    "    def language_match_func(row):\n",
    "        if pd.isna(row['languages_spoken']) or pd.isna(row['preferred_provider_language']):\n",
    "            return 0\n",
    "        spoken = [lang.strip() for lang in str(row['languages_spoken']).split(';')]\n",
    "        return 1 if row['preferred_provider_language'] in spoken else 0\n",
    "\n",
    "    inference_df['language_match'] = inference_df.apply(language_match_func, axis=1)\n",
    "\n",
    "    # Geographic Feature - CRITICAL: Use the same scaling as the training data\n",
    "    dist = pgeocode.GeoDistance('US')\n",
    "    inference_df['distance_km'] = dist.query_postal_code(\n",
    "        inference_df['zip_code_x'].astype(str).tolist(), \n",
    "        inference_df['zip_code_y'].astype(str).tolist()\n",
    "    )\n",
    "    # Impute missing distances using the same logic (specialty mean, then global mean)\n",
    "    mean_dist_by_specialty = inference_df.groupby('specialty')['distance_km'].transform('mean')\n",
    "    inference_df['distance_km'].fillna(mean_dist_by_specialty, inplace=True)\n",
    "    inference_df['distance_km'].fillna(training_max_dist / 2, inplace=True) # Fallback with a reasonable value\n",
    "\n",
    "    inference_df['proximity_score'] = 1 - (\n",
    "        (inference_df['distance_km'] - training_min_dist) / (training_max_dist - training_min_dist)\n",
    "    )\n",
    "    # Clip scores to be between 0 and 1, in case a new distance is outside the training range\n",
    "    inference_df['proximity_score'] = inference_df['proximity_score'].clip(0, 1)\n",
    "\n",
    "    inference_df['cultural_competency_rating_prov'] = inference_df['cultural_competency_rating']\n",
    "    # --- Step 4: Predict Scores ---\n",
    "    # Ensure the feature list matches the one used for training\n",
    "    features = [\n",
    "        'years_experience', 'cultural_competency_rating_prov', 'communication_rating',\n",
    "        'race_match', 'ethnicity_match', 'language_match', 'proximity_score', \n",
    "        'interpreter_services_24_7'\n",
    "    ]\n",
    "    X_inference = inference_df[features]\n",
    "    predicted_scores = model_to_use.predict(X_inference)\n",
    "    inference_df['predicted_success_score'] = predicted_scores\n",
    "\n",
    "    # --- Step 5: Rank and Return ---\n",
    "    recommendations = inference_df.sort_values(by='predicted_success_score', ascending=False)\n",
    "\n",
    "    print(\"--- Recommendations Generated ---\")\n",
    "   # print(\"--- Recommendations Generated ---\")\n",
    "    return recommendations[['provider_id', 'first_name_x', 'last_name_x', 'specialty', 'hospital_name','distance_km', 'predicted_success_score']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcf034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Recommendation Phase for Patient ID: PAT_000001 ---\n",
      "Patient preference: 'No Specific Preference'. Routing to 'No Specific Preference' model.\n",
      "inference_df columns: ['provider_id', 'npi_number', 'first_name_x', 'last_name_x', 'specialty', 'practice_zip_code', 'years_experience', 'medical_school_country', 'board_certified', 'languages_spoken', 'interpreter_services', 'cultural_certifications', 'minority_health_experience', 'community_involvement', 'patient_satisfaction_score', 'communication_rating', 'cultural_competency_rating', 'hospital_affiliation', 'accepts_new_patients', 'provider_race', 'provider_ethnicity', 'patient_id', 'first_name_y', 'last_name_y', 'date_of_birth', 'gender', 'race', 'ethnicity', 'primary_language', 'zip_code_x', 'insurance_type', 'household_income', 'education_level', 'age', 'cultural_background', 'preferred_provider_language', 'cultural_preferences', 'hospital_id', 'hospital_name', 'hospital_type', 'zip_code_y', 'bed_count', 'teaching_hospital', 'trauma_center', 'language_services_available', 'cultural_competency_program', 'interpreter_services_24_7', 'community_health_programs', 'overall_rating', 'patient_safety_rating', 'readmission_rate']\n",
      "--- Recommendations Generated ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider_id</th>\n",
       "      <th>first_name_x</th>\n",
       "      <th>last_name_x</th>\n",
       "      <th>specialty</th>\n",
       "      <th>hospital_name</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>predicted_success_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>PROV_02450</td>\n",
       "      <td>John</td>\n",
       "      <td>Hernandez</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>Metro Medical Center</td>\n",
       "      <td>8158.457169</td>\n",
       "      <td>0.905949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>PROV_03111</td>\n",
       "      <td>Ahmed</td>\n",
       "      <td>Lee</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>General Medical Complex</td>\n",
       "      <td>8158.457169</td>\n",
       "      <td>0.899970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>PROV_02174</td>\n",
       "      <td>Antonio</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>General Hospital</td>\n",
       "      <td>8158.457169</td>\n",
       "      <td>0.885381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>PROV_03737</td>\n",
       "      <td>Sofia</td>\n",
       "      <td>Mohamed</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>Community Medical Complex</td>\n",
       "      <td>8158.457169</td>\n",
       "      <td>0.885328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>PROV_02179</td>\n",
       "      <td>Wei</td>\n",
       "      <td>Lopez</td>\n",
       "      <td>Cardiology</td>\n",
       "      <td>Memorial Medical Center</td>\n",
       "      <td>8158.457169</td>\n",
       "      <td>0.883657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider_id first_name_x last_name_x   specialty  \\\n",
       "143  PROV_02450         John   Hernandez  Cardiology   \n",
       "173  PROV_03111        Ahmed         Lee  Cardiology   \n",
       "119  PROV_02174      Antonio       Brown  Cardiology   \n",
       "203  PROV_03737        Sofia     Mohamed  Cardiology   \n",
       "121  PROV_02179          Wei       Lopez  Cardiology   \n",
       "\n",
       "                 hospital_name  distance_km  predicted_success_score  \n",
       "143       Metro Medical Center  8158.457169                 0.905949  \n",
       "173    General Medical Complex  8158.457169                 0.899970  \n",
       "119           General Hospital  8158.457169                 0.885381  \n",
       "203  Community Medical Complex  8158.457169                 0.885328  \n",
       "121    Memorial Medical Center  8158.457169                 0.883657  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "# First, get the min and max distance from your *full* training data to pass to the function\n",
    "min_dist_training = master_df['distance_km'].min()\n",
    "max_dist_training = master_df['distance_km'].max()\n",
    "\n",
    "# Replace with a real patient_id and specialty from your data\n",
    "example_patient_id = 'PAT_000001' # Replace with a valid ID\n",
    "example_specialty = 'Cardiology' # Replace with a valid specialty\n",
    "\n",
    "# Ensure that there are models trained before running this\n",
    "if trained_models:\n",
    "    final_recommendations = get_recommendations(\n",
    "        patient_id=example_patient_id,\n",
    "        required_specialty=example_specialty,\n",
    "        all_providers_df=provider_df,\n",
    "        all_patients_df=patient_df,\n",
    "        all_hospitals_df=hospital_df,\n",
    "        models_dict=trained_models,\n",
    "        training_min_dist=min_dist_training,\n",
    "        training_max_dist=max_dist_training\n",
    "    )\n",
    "    display(final_recommendations.head(5))\n",
    "    \n",
    "else:\n",
    "    print(\"No models were trained, cannot generate recommendations.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243aa7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c075483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fb2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a89de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env2709_Capstone_py_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
